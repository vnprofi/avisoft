from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import csv
import re
import time
import random

BASE_URL = "https://www.avito.ru"

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]


def get_random_headers():
    """Генерирует случайные заголовки"""
    return {
        "X-Forwarded-For": f"{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}",
        "X-Real-IP": f"{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}",
        "CF-Connecting-IP": f"{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}"
    }


def create_browser_context(playwright_instance):
    """Создает контекст браузера с антибан настройками"""
    browser = playwright_instance.chromium.launch(
        headless=False,
        args=[
            '--disable-blink-features=AutomationControlled',
            '--disable-web-security',
            '--disable-features=VizDisplayCompositor',
            '--no-sandbox',
            '--disable-dev-shm-usage'
        ]
    )
    context = browser.new_context(
        viewport={"width": 1920, "height": 1080},
        user_agent=random.choice(USER_AGENTS),
        locale="ru-RU",
        timezone_id="Europe/Moscow",
        extra_http_headers=get_random_headers(),
        java_script_enabled=True,
        bypass_csp=True,
        ignore_https_errors=True
    )

    page = context.new_page()
    page.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
        Object.defineProperty(navigator, 'languages', {get: () => ['ru-RU', 'ru', 'en-US', 'en']});
        window.chrome = { runtime: {} };
    """)

    return browser, context, page


def extract_seller_products(url):
    """Загружает страницу и возвращает распарсенные данные"""
    with sync_playwright() as p:
        browser, context, page = create_browser_context(p)

        try:
            page.goto(url, timeout=60000, wait_until="domcontentloaded")
            page.wait_for_timeout(3000)

            scroll_attempts = 0
            max_attempts = 50

            while scroll_attempts < max_attempts:
                current_items = len(page.query_selector_all('[data-marker="item"], div[class*="iva-item-root"]'))
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(500)
                new_items = len(page.query_selector_all('[data-marker="item"], div[class*="iva-item-root"]'))

                if new_items == current_items:
                    scroll_attempts += 1
                else:
                    scroll_attempts = 0

                print(f"Загружено товаров: {new_items}")

                if scroll_attempts >= 3:
                    break

            try:
                expand_links = page.query_selector_all('a[data-marker="expand-text"]')
                for link in expand_links:
                    try:
                        link.click()
                        page.wait_for_timeout(500)
                    except:
                        pass
            except:
                pass

            html = page.content()
        finally:
            browser.close()

    return extract_from_html(html)


def extract_product_details_xpath(page, url):
    """БЫСТРОЕ извлечение через XPath"""
    try:
        page.goto(url, timeout=30000, wait_until="domcontentloaded")
        page.wait_for_timeout(1500)  # Сократили до 1.5 сек

        # Скролл для триггера загрузки элементов
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        page.wait_for_timeout(500)

        # БЫСТРОЕ раскрытие прайс-листа - кликаем сразу
        try:
            # Принудительный клик через JavaScript
            page.evaluate("""
                const buttons = document.querySelectorAll('[data-marker*="PRICE_LIST_TITLE_MARKER"], .gVNL7 ._o8T3');
                buttons.forEach(btn => {
                    if (btn) {
                        btn.click();
                        btn.dispatchEvent(new Event('click', {bubbles: true}));
                        btn.dispatchEvent(new MouseEvent('mousedown'));
                        btn.dispatchEvent(new MouseEvent('mouseup'));
                    }
                });
            """)

            # Минимальное ожидание раскрытия
            page.wait_for_timeout(800)

        except Exception as e:
            print(f"Ошибка раскрытия прайс-листа: {e}")

        # Извлечение данных
        result = {
            "location_detail": extract_by_xpath(page,
                                                "//div[@class='F3kIg']//h2[contains(text(), 'Расположение')]/following-sibling::div[@class='ljYEJ']"),
            "details": extract_by_xpath(page, "//div[@id='bx_item-params']"),
            "description": extract_by_xpath(page, "//div[@id='bx_item-description']"),
            "additional": extract_by_xpath(page, "//div[@class='UaGSK']"),
            "price_list": extract_price_list_xpath(page)
        }

        return result

    except Exception as e:
        print(f"Ошибка при парсинге карточки {url}: {e}")
        return {"location_detail": "", "details": "", "price_list": "", "description": "", "additional": ""}


def extract_product_details_xpath(page, url):
    """БЫСТРОЕ извлечение через XPath"""
    try:
        page.goto(url, timeout=30000, wait_until="domcontentloaded")
        page.wait_for_timeout(1500)  # Сократили до 1.5 сек

        # Скролл для триггера загрузки элементов
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        page.wait_for_timeout(500)

        # БЫСТРОЕ раскрытие прайс-листа с проверкой
        try:
            for attempt in range(3):  # Максимум 3 попытки
                # Клик по всем кнопкам прайс-листа
                page.evaluate("""
                    const buttons = document.querySelectorAll('[data-marker*="PRICE_LIST_TITLE_MARKER"], .gVNL7 ._o8T3, .gVNL7 .button');
                    console.log('Найдено кнопок:', buttons.length);
                    buttons.forEach((btn, i) => {
                        console.log('Кликаем кнопку', i, btn.className);
                        btn.click();
                        btn.dispatchEvent(new Event('click', {bubbles: true}));
                    });
                """)

                page.wait_for_timeout(600)

                # Проверяем, появились ли услуги
                price_items = page.query_selector_all('[data-marker*="PRICE_LIST_VALUE_MARKER"]')
                print(f"Попытка {attempt + 1}: найдено {len(price_items)} услуг")

                if len(price_items) > 0:
                    break

            if len(price_items) == 0:
                print("Прайс-лист не раскрылся, пробуем альтернативный способ...")
                # Альтернативный клик по любому элементу с "Прайс"
                page.evaluate("""
                    const elements = document.querySelectorAll('*');
                    for (let el of elements) {
                        if (el.textContent && el.textContent.includes('Прайс')) {
                            el.click();
                            break;
                        }
                    }
                """)
                page.wait_for_timeout(800)

        except Exception as e:
            print(f"Ошибка раскрытия прайс-листа: {e}")

        # Извлечение данных
        result = {
            "location_detail": extract_by_xpath(page,
                                                "//div[@class='F3kIg']//h2[contains(text(), 'Расположение')]/following-sibling::div[@class='ljYEJ']"),
            "details": extract_by_xpath(page, "//div[@id='bx_item-params']"),
            "description": extract_by_xpath(page, "//div[@id='bx_item-description']"),
            "additional": extract_by_xpath(page, "//div[@class='UaGSK']"),
            "price_list": extract_price_list_xpath(page)
        }

        return result

    except Exception as e:
        print(f"Ошибка при парсинге карточки {url}: {e}")
        return {"location_detail": "", "details": "", "price_list": "", "description": "", "additional": ""}


def extract_price_list_xpath(page):
    """Улучшенное извлечение прайс-листа через XPath"""
    try:
        result_parts = []

        # Заголовок прайс-листа
        title_xpath = "//h2[@class='EEPdn' and contains(text(), 'Прайс-лист')]"
        title = extract_by_xpath(page, title_xpath)
        if title:
            result_parts.append(f"=== {title} ===")

        # Извлечение всех услуг
        services_xpath = "//div[@data-marker[contains(., 'PRICE_LIST_VALUE_MARKER')]]"

        try:
            services = page.locator(f"xpath={services_xpath}")
            count = services.count()
            print(f"Найдено {count} услуг в прайс-листе")

            for i in range(count):
                service = services.nth(i)

                # Название услуги - более точный селектор
                name = ""
                try:
                    name_elem = service.locator(
                        "xpath=.//p[contains(@class, 'T7ujv') and contains(@class, 'Tdsqf')]").first
                    if name_elem.count() > 0:
                        name = name_elem.inner_text(timeout=1000)
                except:
                    pass

                # Цена
                price = ""
                try:
                    price_elem = service.locator("xpath=.//strong[@class='OVzrF']").first
                    if price_elem.count() > 0:
                        price = price_elem.inner_text(timeout=1000)
                except:
                    pass

                if name:
                    service_line = f"• {clean_text_simple(name)}"
                    if price:
                        service_line += f": {clean_text_simple(price)}"
                    result_parts.append(service_line)

        except Exception as e:
            print(f"Ошибка извлечения услуг: {e}")

        # Если структурированно ничего не найдено - берем весь блок
        if len(result_parts) <= 1:
            full_block_xpath = "//div[@class='gVNL7']"
            full_text = extract_by_xpath(page, full_block_xpath)
            if full_text and len(full_text) > 20:
                result_parts.append("Полный текст прайс-листа:")
                result_parts.append(full_text)

        return "\n".join(result_parts)

    except Exception as e:
        print(f"Ошибка извлечения прайс-листа: {e}")
        return ""


def extract_by_xpath(page, xpath_expr):
    """Быстрое извлечение текста по XPath"""
    try:
        element = page.locator(f"xpath={xpath_expr}").first
        if element.count() > 0:
            text = element.inner_text(timeout=2000)
            return clean_text_simple(text)
    except:
        pass
    return ""


def extract_price_list_xpath(page):
    """Быстрое извлечение прайс-листа через XPath"""
    try:
        result_parts = []

        # Заголовок прайс-листа
        title_xpath = "//div[@class='gVNL7']//h2[@class='EEPdn']"
        title = extract_by_xpath(page, title_xpath)
        if title:
            result_parts.append(f"=== {title} ===")

        # XPath для всех элементов прайс-листа
        services_xpath = "//div[@data-marker[contains(., 'PRICE_LIST_VALUE_MARKER')]]"

        try:
            services = page.locator(f"xpath={services_xpath}")
            count = services.count()
            print(f"Найдено {count} услуг в прайс-листе")

            for i in range(count):
                service = services.nth(i)

                # Название услуги
                name_xpath = ".//p[contains(@class, 'T7ujv') and contains(@class, 'Tdsqf')]"
                name = ""
                try:
                    name_elem = service.locator(f"xpath={name_xpath}").first
                    if name_elem.count() > 0:
                        name = name_elem.inner_text(timeout=1000)
                except:
                    pass

                # Цена
                price_xpath = ".//strong[@class='OVzrF']"
                price = ""
                try:
                    price_elem = service.locator(f"xpath={price_xpath}").first
                    if price_elem.count() > 0:
                        price = price_elem.inner_text(timeout=1000)
                except:
                    pass

                if name:
                    service_line = f"• {clean_text_simple(name)}"
                    if price:
                        service_line += f": {clean_text_simple(price)}"
                    result_parts.append(service_line)

        except Exception as e:
            print(f"Ошибка извлечения услуг: {e}")

        # Если ничего не найдено структурированно - берем весь текст блока
        if len(result_parts) <= 1:
            full_block_xpath = "//div[@class='gVNL7']"
            full_text = extract_by_xpath(page, full_block_xpath)
            if full_text and len(full_text) > 20:
                result_parts.append("Полный текст прайс-листа:")
                result_parts.append(full_text)

        return "\n".join(result_parts)

    except Exception as e:
        print(f"Ошибка извлечения прайс-листа: {e}")
        return ""


def clean_text_simple(text):
    """Быстрая очистка текста"""
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text.replace('\xa0', ' ')).strip()


def extract_from_html(html_content: str):
    """Парсит страницу и собирает товары + инфо о продавце"""
    soup = BeautifulSoup(html_content, "html.parser")

    products = []
    item_cards = soup.select('[data-marker="item"], div[class*="iva-item-root"]')

    for i, card in enumerate(item_cards, start=1):
        title_a = card.select_one('a[data-marker="item-title"]')
        if not title_a:
            continue

        name = title_a.get_text(strip=True)
        href = title_a.get("href", "")
        url = urljoin(BASE_URL, href)

        price_value = extract_price(card)
        location_value = extract_location(card)
        date_value = extract_date(card)

        products.append({
            "index": i,
            "name": name,
            "url": url,
            "title": title_a.get("title", ""),
            "price": price_value,
            "location": location_value,
            "date": date_value,
        })

    seller_info = extract_seller_info(soup)

    return {
        "total_products": len(products),
        "products": products,
        "seller_info": seller_info,
    }


def clean_text(node) -> str:
    if not node:
        return ""
    for svg in node.find_all("svg"):
        svg.decompose()
    return " ".join(node.stripped_strings).replace("\xa0", " ").strip()


def extract_price(container) -> str:
    if not container:
        return ""

    meta_price = container.select_one('meta[itemprop="price"]')
    if meta_price and meta_price.get("content"):
        return meta_price["content"].strip()

    price_p = container.select_one('p[data-marker="item-price"]')
    if price_p:
        return clean_text(price_p)

    txt = clean_text(container)
    m = re.search(r"\d[\d\s]*₽", txt)
    if m:
        return m.group(0).strip()

    return ""


def extract_location(container) -> str:
    if not container:
        return ""

    geo = container.select_one('div[class*="geo-root"]')
    if geo:
        return clean_text(geo)

    return ""


def extract_date(container) -> str:
    """Извлекает дату размещения объявления"""
    if not container:
        return ""
    date_p = container.select_one('p[data-marker="item-date"]')
    if date_p:
        return clean_text(date_p)
    return ""


def extract_seller_info(soup):
    """Извлекает информацию о продавце"""
    seller_info = {
        "name": "",
        "rating": "",
    }

    name_wrap = soup.find("div", class_=re.compile(r"AvatarNameView-name"))
    if name_wrap:
        h1 = name_wrap.find(["h1", "h2"])
        if h1:
            seller_info["name"] = h1.get_text(strip=True)

    rating_el = soup.find("span", {"data-marker": "profile/score"})
    if rating_el:
        seller_info["rating"] = rating_el.get_text(strip=True)

    return seller_info


def save_to_csv(data, filename="avito_products.csv", detailed=False):
    if not data.get("products"):
        print("Нет данных для сохранения")
        return

    with open(filename, "w", newline="", encoding="utf-8") as f:
        if detailed:
            fieldnames = [
                "index", "name", "url", "title", "price", "location", "date",
                "location_detail", "details", "price_list", "description", "additional", "seller_name", "seller_rating",
            ]
        else:
            fieldnames = [
                "index", "name", "url", "title", "price", "location", "date",
                "seller_name", "seller_rating",
            ]

        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        s = data.get("seller_info", {}) or {}
        for row in data["products"]:
            row_out = dict(row)
            row_out.update({
                "seller_name": s.get("name", ""),
                "seller_rating": s.get("rating", ""),
            })
            writer.writerow(row_out)

    print(f"Данные сохранены в {filename}")


def main():
    url = "https://www.avito.ru/brands/0659057cfce2a2ec703097e7e72ce397/all/predlozheniya_uslug?src=search_seller_info&iid=7584491465&sellerId=44fb53866f13711eda78418a617f6242"

    print("Начинаем парсинг товаров…")
    result = extract_seller_products(url)

    print("\nИнформация о продавце:")
    for k, v in result.get("seller_info", {}).items():
        print(f"{k}: {v}")

    print(f"\nНайдено товаров: {result['total_products']}")
    for product in result["products"]:
        print(
            f"\n{product['index']}. {product['name']}\n"
            f"Цена: {product['price']}\n"
            f"Локация: {product['location']}\n"
            f"Дата: {product['date']}\n"
            f"URL: {product['url']}\n"
            f"Title: {product['title']}\n"
            + "-" * 50
        )

    while True:
        choice = input("\nСобирать данные из карточки каждого объявления? (да/нет): ").strip().lower()
        if choice in ['да', 'yes', 'y', 'д']:
            print("Начинаем БЫСТРЫЙ сбор детальной информации через XPath...")

            # Создаем один браузер для всех карточек
            with sync_playwright() as p:
                browser, context, page = create_browser_context(p)

                try:
                    start_time = time.time()
                    for i, product in enumerate(result["products"], 1):
                        print(f"Обрабатываем карточку {i}/{len(result['products'])}: {product['name']}")

                        # БЫСТРАЯ функция через XPath
                        details = extract_product_details_xpath(page, product["url"])
                        product["location_detail"] = details["location_detail"]
                        product["details"] = details["details"]
                        product["price_list"] = details["price_list"]
                        product["description"] = details["description"]
                        product["additional"] = details["additional"]

                        # СОКРАЩЕННАЯ пауза
                        time.sleep(random.uniform(1.5, 3))  # Было 3-7 секунд

                    elapsed = time.time() - start_time
                    print(f"\n⚡ Обработка завершена за {elapsed:.1f} секунд!")

                finally:
                    browser.close()

            save_to_csv(result, detailed=True)
            print("Сбор детальной информации завершен!")
            break

        elif choice in ['нет', 'no', 'n', 'н']:
            save_to_csv(result, detailed=False)
            print("Работа завершена!")
            break

        else:
            print("Пожалуйста, введите 'да' или 'нет'")


if __name__ == "__main__":
    main()
